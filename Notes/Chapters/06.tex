\chapter{Diffusion Processes}

This chapter studies the properties of solutions of stochastic differential equations. Since these solutions can be interpreted as in the physical Brownian motion, such stochastic processes are called \textbf{(Itô) diffusions}. 

\section{Basic Definitions}

Throughout this chapter, consider a stochastic differential equation 
\begin{equation}\label{eq:202301231650}
    \mathrm{d} X_t = \mu(t, X_t) \mathrm{d}t + \sigma(t, X_t) \mathrm{d}B_t
\end{equation}
with $X_t \in \textbf{R}^n$, $\mu(t,x) \in \textbf{R}^n$, $\sigma(t,x) \in \textbf{R}^{n \times m}$ and $B_t$ is $m$-dimensional Brownian motion. We call $\mu$ the \textbf{drift coefficient} and $\sigma$ (or $\frac{1}{2} \sigma \sigma^T$) the \textbf{diffusion coefficient}. 

\begin{definition}[Itô Diffusion]
    A (time-homogeneous) \textbf{Itô diffusion} is a stochastic process 
    \[
        X_t(\omega) : [0, \infty] \times \Omega \longrightarrow \textbf{R}^n
    \]
    satisfying a stochastic differential equation of the form 
    \begin{equation*}
        \mathrm{d} X_t = \mu(X_t) \mathrm{d}t + \sigma(X_t) \mathrm{d}B_t, \quad t \geq s, \quad X_s = x
    \end{equation*}
    satisfying the conditions of the Theorem \ref{thm:existence_uniqueness}, which in this case simplify to: 
    \begin{equation*}
        |\mu(x) - \mu(y)| + |\sigma(x) - \sigma(y)| \leq D|x-y|, \quad x, y \in \textbf{R}^n
    \end{equation*}
    with $|\sigma|^2 = \sum |\sigma_{ij}|^2$. 

    We denote the solution as $X_t = X_t^{s,x}$ and, for $s = 0$, we write $X_t^x = X_t^{0,x}$.
\end{definition}

Notice that the process $X_t(\omega)$ has the property of being \textbf{time-homogeneous}, i.e. 
\begin{equation*}
    \begin{aligned}
        X_{s+h}^{s,x} &= x + \int_s^{s+h} \mu(X_u^{s,x})~\mathrm{d}u + \int_s^{s+h} \mu(X_u^{s,x})~\mathrm{d}B_u \\
        &= x + \int_0^{h} \mu(X_{s+v}^{s,x})~\mathrm{d}v + \int_0^{h} \mu(X_{s+v}^{s,x})~\mathrm{d}\overline{B}_v
    \end{aligned} 
\end{equation*}
with $u = s+v$, $\overline{B}_v = B_{s+v} - B_s$, $v \geq 0$. 

Taking $s = 0$, 
\[
    X_{h}^{0,x} = x + \int_0^{h} \mu(X_{v}^{0,x})~\mathrm{d}v + \int_0^{h} \mu(X_{v}^{0,x})~\mathrm{d}B_v
\]

Since the processes $\{ \overline{B}_v \}$ and $\{ B_v \}$ have the same $P^0$-distribution, by weak uniqueness of the solution, we have that $\{ X_{s+h}^{s,x} \}_{h \geq 0}$ and $\{ X_h^{0,x} \}_{h \geq 0}$ have the same $P^0$-distributions. Thus, $\{ X_t \}_{t \geq 0}$ is time-homogeneous. 

Let us now introduce new probability laws $Q^x$ that give the distribution of $\{ X_t \}_{t \geq 0}$ assuming that $X_0 = x$. Let $\mathfrak{M}_\infty$ be the $\sigma$-algebra generated by the random variables $\omega \longrightarrow X_t(\omega) = X_t^y(\omega)$, with $t \geq 0$ and $y \in \textbf{R}^n$. We define $Q^x$ on $\mathfrak{M}$ as
\[
    Q^x [X_{t_1} \in E_1, \ldots, X_{t_k} \in E_k] = P^0[X_{t_1}^x \in E_1, \ldots, X_{t_k}^x \in E_k]
\]
where $E_1 \subset \textbf{R}^n$ are Borel sets and $1 \leq i \leq k$. 

Also let $\mathfrak{F}_t^{(m)}$ be the $\sigma$-algebra generated by $\{ B_r : r \leq t\}$ and $\mathfrak{M}_t$ be the $\sigma$-algebra generated by $\{ X_r : r \leq t\}$. Since $X_t$ is measurable w.r.t. $\mathfrak{F}_t^{(m)}$, it follows that $\mathfrak{M}_t \subseteq \mathfrak{F}_t^{(m)}$.

\section{The Markov Property}

Another feature of the solution $X_t$ is the Markov property, which says that, given the present state, the future is independent of the past. Put another way, given what happened up to time $t$, the future of the process is the same as the behavior obtained by starting the process at $X_t$. More precisely, 

\begin{definition}[Markov Property]
    For any $0 \leq s < t$,
    \[
        P[X_t \leq y \mid \mathfrak{F}_s] = P[X_t \leq y \mid X_s] \quad \text{ a.s. }
    \]
\end{definition}

The next results states that a solution to an SDE $X_t$ is a Markov process w.r.t. $\{ \mathfrak{F}_t^{(m)} \}_{t \geq 0}$. 

\begin{theorem}[Markov Property for Itô Diffusions]\label{thm:markov_prop_ito}
    Let $f : \textbf{R}^n \longrightarrow \textbf{R}$ be a bounded Borel function. For $t, h \geq 0$, 
    \begin{equation}\label{eq:markov_property}
        \mathbb{E}^x [f(X_{t+h}) \mid \mathfrak{F}_t^{(m)}]_{(\omega)} = \mathbb{E}^{X_t(\omega)} [f(X_h)]
    \end{equation}
    where $\mathbb{E}^x$ denotes the expected value w.r.t. the probability measure $Q^x$, i.e., $\mathbb{E}^x[f(X_h)] = \mathbb[f(X_h^x)]$ w.r.t. the probability measure $P^0$. 
\end{theorem}

\begin{proof}
    By uniqueness (of SDE solutions), $X_r(\omega) = X_r^{t, X_t}(\omega)$. 

    Define 
    \[
        F(x, t, r, \omega) = X_r^{t,x}(\omega), \quad r \geq t
    \]
    and use this to write $X_r(\omega) = F(X_t, t, r, \omega)$. 

    Using that $\omega \longrightarrow F(x, t, r, \omega)$ is $\mathfrak{F}_t^{(m)}$-independent and the time-homogeneity, we have that 
    % that $\{ X_{t+h}^{t,X_t} \}$ and $\{ X_h^{0,X_t} \}$ have the same distributions
    \[
        \mathbb{E}[f(F(X_t, t, t+h, \omega)) \mid \mathfrak{F}_t^{(m)}] = \mathbb{E}[f(F(X_t, 0, h, \omega))]
    \]

    Let $g(t, \omega) = f \circ F(x, t, t+h, \omega)$ and notice that it is measurable. Thus, we can approximate $g$ pointwise boundedly by 
    \[
        \sum_{k=1}^{m} \varphi_k(x) \psi_k(\omega)
    \]

    Compute 
    \[
        \mathbb{E}[g(X_t, \omega) \mid \mathfrak{F}_t^{(m)}] = \mathbb{E}[g(X_t, \omega)]
    \]

    Using that $\{ X_r \}$ is time-homogeneous, the result follows. 
\end{proof}

% Solutions to SDEs have the Markov property. Indeed, by the construction of the weak solution (what is the construction of the solution in the canonical space?), the Markov property holds.

\begin{remark}
    Since $\mathfrak{M}_t \subseteq \mathfrak{F}_t^{(m)}$, we have that $X_t$ is also a Markov process w.r.t. $\{ \mathfrak{M}_t \}_{t \geq 0}$. With the fact $\mathbb{E}^{X_t} [f(X_{h})]$ is $\mathfrak{M}_t$-measurable and the Theorem \ref{thm:cond_exp_2} items 3 and 6, it follows that 
    \begin{equation*}
        \begin{aligned}
            \mathbb{E}^x [ f(X_{t+h}) \mid \mathfrak{M}_t] &= \mathbb{E}^x[ \mathbb{E}^x [f(X_{t+h}) \mid \mathfrak{F}_t^{(m)}] \mid \mathfrak{M}_t] \\
            &= \mathbb{E}^x[\mathbb{E}^{X_t} [f(X_{h})] \mid \mathfrak{M}_t] \\ 
            &= \mathbb{E}^{X_t} [f(X_{h})]
        \end{aligned}
    \end{equation*}
\end{remark}

What happens if we replace the time $t$ in \eqref{eq:markov_property} by a random time $\tau(\omega)$, i.e., a stopping time? The Strong Markov Property states that the equation \eqref{eq:markov_property} continues to hold, but before proving it, we need to make precise some terms.

To define a stopping time, the intuition is that we should be able to verify whether or not $\tau \leq t$ has occurred using the information contained in an increasing family of $\sigma$-algebras. 

\begin{definition}[Stopping time]
    Let $\{ \mathfrak{N}_t \}$ be an increasing family of $\sigma$-algebras of subsets of $\Omega$. A (strict) \textbf{stopping time} (also called a \textbf{Markov time}) w.r.t. $\{ \mathfrak{N}_t \}$ is a function $\tau : \Omega \longrightarrow [0, \infty]$ such that $\{ \omega : \tau(\omega) \leq t \} \in \mathfrak{N}_t$ for all $t \geq 0$.
\end{definition}

\begin{definition}[First Exit Time]
    Given $H \subset \textbf{R}^n$, we define the \textbf{first exist time} from $H$ as 
    \[
        \tau_H = \inf \{ t > 0 : X_t \notin H \}
    \]
\end{definition}

\begin{remark}
    The family $\{ \mathfrak{M}_t \}$ is right-continuous and $\tau_H$ is a stopping time for any Borel set $H$ (assuming that the sets of measure zero are included in $\mathfrak{M}_t$).
\end{remark}

\begin{definition}
    Let $\tau$ be a stopping time w.r.t. $\{ \mathfrak{N}_t \}$, and let $\mathfrak{N}_\infty$ be the smallest $\sigma$-algebra containing $\mathfrak{N}_t$ for all $t \geq 0$. Then the $\sigma$-algebra $\mathfrak{N}_t$ consists of all sets $N \in \mathfrak{N}_\infty$ such that 
    \[
        N \bigcap \{ \tau \leq t \} \in \mathfrak{N}_t, \quad \forall ~t\geq 0
    \]
\end{definition}

\begin{remark}
    If $\mathfrak{N}_t = \mathfrak{M}_t$, we can describe $\mathfrak{M}_t$ as the $\sigma$-algebra generated by $\{ X_{\min(s, \tau)} : s \geq 0 \}$.

    If $\mathfrak{N}_t = \mathfrak{F}_t^{(m)}$, we can describe $\mathfrak{F}_t^{(m)}$ as the $\sigma$-algebra generated by $\{ B_{s \wedge \tau} : s \geq 0 \}$.
\end{remark}

\begin{theorem}[Strong Markov Property for Itô Diffusions]
    Let $f$ be a bounded Borel function defined on $\textbf{R}^n$, $\tau$ a stopping time w.r.t. $\mathfrak{F}_t^{(m)}$, $\tau < \infty$ a.s. then 
    \begin{equation}\label{eq:str_markov_property}
        \mathbb{E}^x [f(X_{\tau+h}) \mid \mathfrak{F}_\tau^{(m)}]_{(\omega)} = \mathbb{E}^{X_\tau(\omega)} [f(X_h)], \quad \forall ~h \geq 0
    \end{equation}
\end{theorem}

\begin{proof}
    The idea here is to imitate the proof of the Theorem \ref{thm:markov_prop_ito}.
\end{proof}

The equation \eqref{eq:str_markov_property} can be extended as follows.

\begin{corollary}
    If $f_1, \ldots, f_k$ are bounded Borel functions on $\textbf{R}^n$, $\tau$ a stopping time w.r.t. $\mathfrak{F}_t^{(m)}$, $\tau < \infty$ a.s. then 
    \begin{equation}\label{eq:202301301710}
        \mathbb{E}^x [f_1(X_{\tau+h_1}) f_2(X_{\tau+h_2}) \cdots f_k(X_{\tau+h_k}) \mid \mathfrak{F}_t^{(m)}] = \mathbb{E}^{X_\tau} [f_1(X_{\tau+h_1}) f_2(X_{\tau+h_2}) \cdots f_k(X_{\tau+h_k})]
    \end{equation}
\end{corollary}

\begin{proof}
    By induction on $k$.
\end{proof}

\subsection{Hitting distribution and Harmonic measure}

We end this section by applying the concept of the shift operator to the expected hitting time to a boundary and the probabilistic representation of Harmonic functions. 

\begin{definition}[Shift Operator]
    Let $\mathfrak{H}$ be the set of all real $\mathfrak{M}_\infty$-measurable functions, and $t \geq 0$. The \textbf{shift operator} 
    \[ 
        \theta_t : \mathfrak{H} \longrightarrow \mathfrak{H}
    \]
    as follows. 

    If $\eta = g_1(X_{t_1}) \cdots g_k(X_{t_k})$, with each $g_i$ Boreal measurable and $t_i \geq 0$, we define 
    \[
        \theta_t \eta = g_1(X_{t_1 + t}) \cdots g_k(X_{t_k+t})
    \]

    And we extend to all functions in $\mathfrak{H}$ in a natural way: by taking limits of sums of such functions. 
\end{definition}

It follows from \eqref{eq:202301301710} that 
\begin{equation}\label{eq:202301301750}
    \mathbb{E}^x [\theta_\tau \eta \mid \mathfrak{F}_\tau^{(m)}] = \mathbb{E}^{X_\tau} [ \eta ]
\end{equation}
for all stopping times $\tau$ and all bounded $\eta \in \mathfrak{H}$, with 
\[
    (\theta_\tau \eta)(\omega) = (\theta_t \eta)(\omega), \quad \text{ if } \tau(\omega) = t
\]

\begin{theorem}
    Let $H \subset \mathbf{R}^n$ be measurable, $\tau_H$ be the first exit time from $H$ for an Itô diffusion $X_t$, $\alpha$ another stopping time, $g$ a bounded continuous function on $\mathbf{R}^n$ and define 
    \[
        \eta = g(X_{\tau_H}) \chi_{\{ \tau_H < \infty \}}, \quad \tau_H^\alpha = \inf \{ t > \alpha : X_t \notin H \}
    \]
    
    Then 
    \begin{equation*}
        \theta_\alpha \eta \chi_{\{ \alpha < \infty \}} = g(X_{\tau_H^\alpha}) \chi_{\{ \tau_H^\alpha < \infty\}}
    \end{equation*}
\end{theorem}

\begin{proof}
    First approximate $\eta$ by functions $\eta^{(k)}$.

    Then compute $\theta_t \chi_{[t_J, t_{j+1})}(\tau_H) = \chi_{[t_j+t, t_{j+1}+t]}(\tau_H^t)$. 

    Finally, notice that $\theta_t \eta = g(X_{\tau_H^t}) \chi_{\{ \tau_H^t < \infty \}}$.
\end{proof}

In particular, if $\alpha = \tau_G$, with $G \subset \subset H$ (i.e. $\overline{G}$ is compact and $\overline{G} \subset H$) measurable, and $ < \infty$ a.s. w.r.t. $Q^x$, then $\tau_H^\alpha = \tau_H$ and 
\begin{equation}\label{eq:202301301838}
    \theta_{\tau_G} g(X_{\tau_H}) = g(X_{\tau_H})
\end{equation}

Thus, if $f$ is a bounded measurable function, from \eqref{eq:202301301750} and \eqref{eq:202301301838} we have that 
\begin{equation*}
    \mathbb{E}^x [ f(X_{\tau_H}) ] = \mathbb{E}^x [\mathbb{E}^{X_{\tau_G}} [f(X_{\tau_H})]] = \int_{\partial G} \mathbb{E}^y [f(X_{\tau_H})] Q^x[X_{\tau_G} \in \mathrm{d}y]
\end{equation*}
for $x \in G$.

\begin{definition}[Harmonic Measure]
    The \textbf{harmonic measure} of $X$ on $\partial G$, denoted by $\mu_G^x$, is defined as 
    \[
        \mu_G^x(F) = Q^x[X_{\tau_G} \in F], \quad \text{ for } F \subset \partial G, ~x \in G
    \]
\end{definition}

The function 
\[
    \varphi(x) = \mathbb{E}^x [ f(X_{\tau_H})]
\]
satisfies the \textbf{mean value property}:
\begin{equation}
    \varphi(x) = \int_{\partial G} \varphi(y) ~\mathrm{d}\mu_G^x(y), \quad \forall~x \in G
\end{equation}
for all Borel sets $G \subset \subset H$.

\section{The Generator of an Itô Diffusion}

In this section, we associate a second order partial differential operator to an Itô diffusion.

\begin{definition}[Generator]
    Let $\{ X_t \}$ be a time-homogeneous Itô diffusion in $\textbf{R}^n$. The (infinitesimal) \textbf{generator} $A$ of $X_t$ is defined by 
    \begin{equation}\label{eq:202301231654}
        A~f(x) = \lim_{t \downarrow 0} \frac{\mathbb{E}^x [ f(X_t)] - f(x)}{t}, \quad x \in \textbf{R}^n
    \end{equation}

    The set of functions $f : \textbf{R}^n \longrightarrow \textbf{R}$ such that the limit exists at $x$ is denoted by $\mathfrak{D}_A(x)$, and $\mathfrak{D}_A$ denotes the set of functions for which the limit exists for all $x \in \textbf{R}^n$.
\end{definition}

Now consider the differential operator associated with the SDE \eqref{eq:202301231650}
\begin{equation*}
    L~f(x) = \sum_i \mu_i(x) \frac{\partial f}{\partial x_i} + \frac{1}{2} \sum_{i, j} (\sigma \sigma^T)_{i,j}(x) \frac{\partial^2 f}{\partial x_i \partial x_j}
\end{equation*}

But how are the generator and the coefficients of the stochastic differential equation related? We'll prove that $A$ and $L$ coincide on $\mathfrak{C}_0^2(\textbf{R}^n)$. To show this, we need the following lemma.

\begin{lemma}\label{lm:202301241653}
    Let $Y_t = Y_t^x$ be an Itô process in $\textbf{R}^n$ of the form 
    \[
        Y_t^x(\omega) = x + \int_0^t u(s,\omega) ~\mathrm{d}s + \int_0^t v(s,\omega) ~\mathrm{d}B_s(\omega)
    \]
    where $B$ is $m$-dimensional.

    And let $f \in \mathfrak{C}_0^2(\textbf{R}^n)$ (notice that this means that $f$ has compact support), $\tau$ be a stopping time w.r.t. $\{ \mathfrak{F}_t^{(m)} \}$ and assume that $\mathbb{E}^x[\tau] < \infty$. Also assume that $u(t, \omega)$ and $v(t, \omega)$ are bounded on the set of $(t, \omega)$ such that $Y(t, \omega)$ belongs to the support of $f$. Then 
    \[
        \mathbb{E}^x[f(Y_\tau)] = f(x) + \mathbb{E}^x\left[ \int_0^\tau \left( \sum_i u_i(s, \omega) \frac{\partial f}{\partial x_i}(Y_s) + \frac{1}{2} \sum_{i,j}(vv^T)_{i,j}(s, \omega) \frac{\partial^2 f}{\partial x_i \partial x_j}(Y_s) \right) ~\mathrm{d}s \right]
    \]
    where $\mathbb{E}^x$ is the expectation w.r.t. the probability law $R^x$ for $Y_t$ starting at x: 
    \[
        R^x[Y_{t_1} \in F_1, \ldots, Y_{t_k} \in F_k] = P^0[Y_{t_1}^x \in F_1, \ldots, Y_{t_k}^x \in F_k], \quad F_i \text{ are Borel sets}
    \]
\end{lemma}

\begin{proof}
    First, apply Itô's formula to $f(Y)$. 

    Notice that $(v \mathrm{d}B)_i (v \mathrm{d}B)_j = (v v^T)_{ij} \mathrm{d}t$ and use this to write an expression for $f(Y_t)$. 

    Compute the expected value of $f(Y_\tau)$.
    
    Using that $g$ is a bounded Borel function, notice that the expected value of the $\mathrm{d}B$ part goes to zero (separate the measurable functions and write the integral as the difference of the measurable and non-measurable parts). 
\end{proof}

\begin{theorem}[Formula for the generator]
    Let $X_t$ be the Itô diffusion 
    \[
        \mathrm{d} X_t = \mu(X_t) \mathrm{d}t + \sigma(X_t) \mathrm{d}B_t
    \]

    If $f \in \mathfrak{C}_0^2(\textbf{R}^n)$, then $f \in \mathfrak{D}_A$ and 
    \begin{equation}\label{thm:202301261616}
        A~f(x) = \sum_i \mu_i(x) \frac{\partial f}{\partial x_i} + \frac{1}{2} \sum_{i, j} (\sigma \sigma^T)_{i,j}(x) \frac{\partial^2 f}{\partial x_i \partial x_j}
    \end{equation}
\end{theorem}

\begin{proof}
    Follows from the Lemma \ref{lm:202301241653}.
\end{proof}

\begin{example}[Standard Brownian Motion]
    The standard $n$-dimensional Brownian motion $B_t$, which satisfies the SDE $ \mathrm{d} X_t = \mathrm{d}B_t$, i.e., with $\mu = 0$ and $\sigma = I_n$, has the generator 
    \[
        A~f = \frac{1}{2} \sum \frac{\partial^2 f}{\partial x_i^2} = \frac{1}{2} \Delta, \quad f = f(x_1, \ldots, x_n) \in \mathfrak{C}_0^2(\textbf{R}^n)
    \]
\end{example}

\begin{example}[Graph of Brownian Motion]
    Consider the two-dimensional process $X_t$ satisfying 
    \[
        \mathrm{d}X_t = \begin{pmatrix}
            \mathrm{d}t \\
            \mathrm{d}B_t
        \end{pmatrix}
    \]
    i.e., with $\mu = \begin{pmatrix}
        1 \\
        0
    \end{pmatrix}$ and $\sigma = \begin{pmatrix}
        0 \\
        1
    \end{pmatrix}$. 

    $X_t$ may be interpreted as the graph of Brownian motion. Its generator is given by 
    \[
        A~f = \frac{\partial f}{\partial t} + \frac{1}{2} \frac{\partial^2 f}{\partial x^2}, \quad f = f(t,x) \in \mathfrak{C}_0^2(\textbf{R}^n)
    \]
\end{example}

\section{Dynkin's Formula}

Using the theory we already have, we can easily draw a useful formula.

\begin{theorem}[Dynkin's Formula]
    Let $f \in \mathfrak{C}_0^2(\textbf{R}^n)$ and suppose that $\tau$ is a stopping time with $\mathbb{E}^x[\tau] < \infty$. Then 
    \begin{equation}\label{thm:dynkin-formula}
        \mathbb{E}^x [f(X_\tau)] = f(x) + \mathbb{E}^x \left[ \int_0^{\tau} A~f(X_s) ~\mathrm{d}s \right]
    \end{equation}
\end{theorem}

\begin{proof}
    Follows from combining Lemma \ref{lm:202301241653} with \eqref{thm:202301261616}.
\end{proof}

Notice that, if $\tau$ is the first exit time of a bounded set, then the result holds for any function $f \in \mathfrak{C}^2$.

\begin{example}
    Consider an $n$-dimensional Brownian motion $B = (B_1, \ldots, B_n)$ starting at $a = (a_1, \ldots, a_n)$ with $|a| < R$. Let us compute the expected value of the first exit time $\tau_K$ of $B$ from the ball 
    \[
        K = K_R = \{ x \in \textbf{R}^n : |x| < R \}
    \]

    Choose an integer $k$ and apply Dynkin's formula with $X = B, \tau = \sigma_k = \min(k, \tau_K)$, and $f \in \mathfrak{C}_0^2$ such that $f(x) = |x|^2$ for $|x| \leq R$: 
    \begin{equation*}
        \begin{aligned}
            \mathbb{E}^a [f(B_{\sigma_k})] &= f(a) + \mathbb{E}^a \left[ \int_0^{\sigma_k} \frac{1}{2} \Delta f(B_s)~\mathrm{d}s \right] \\
            &= |a|^2 + \mathbb{E}^a \left[ \int_0^{\sigma_k} n ~\mathrm{d}s \right] \\
            &= |a|^2 + n \mathbb{E}^a[\sigma_k]
        \end{aligned}  
    \end{equation*}

    Since $\mathbb{E}^a [f(B_{\sigma_k})] \leq R^2$, 
    \[
        \mathbb{E}^a[\sigma_k] \leq \frac{1}{n} (R^2 - |a|^2), \quad \forall ~k
    \]

    Taking $k \to \infty$, we have that $\tau_k = \lim \sigma_k < \infty$ a.s. and 
    \[
        \mathbb{E}^a[\tau_K] = \frac{1}{n} (R^2 - |a|^2)
    \]

    Now assume that $n \geq 2$ and $|b| > R$. We'll compute the probability that $B$ starting at $b$ ever hits the ball $K$.

    Let $\alpha_k$ be the first exit time from the annulus 
    \[
        A_k = \{ x : R < |x| < 2^k R \}, \quad k = 1,2,\ldots 
    \]
    and put 
    \[
        T_K = \inf \{ t > 0 : B_t \in K \}
    \]

    We define $f = f_{n,k} \in \mathfrak{C}^2$ with compact support such that 
    \[
        R \leq |x| \leq 2^k R \implies f(x) = \begin{cases}
            - \log |x|, ~\text{ when } n = 2 \\
            |x|^{2-n}, ~\text{ when } n > 2
        \end{cases}
    \]

    Since $\Delta f = 0$ in $A_k$, by Dynkin's formula, 
    \begin{equation}\label{eq:202301261656}
        \mathbb{E}^b[f(B_{\alpha_k})] = f(b), \quad \forall~k
    \end{equation}

    Define 
    \[
        p_k = P^b[|B_{\alpha_k}| = R] \quad \text{ and } \quad q_k = P^b[|B_{\alpha_k}| = 2^k R]
    \]

    In the case $n = 2$, by \eqref{eq:202301261656} and the definition of $f$, 
    \[
        - \log R \cdot p_k - (\log R + k \log 2) \cdot q_k = - \log |b|, \quad \forall ~k
    \]

    Therefore, $\lim_{k \to \infty} q_k = 0$, and thus 
    \[
        P^b[T_K < \infty] = 1
    \]
    i.e., Brownian motion is \textbf{recurrent} in $\textbf{R}^2$. 

    In the case $n > 2$, 
    \[
        p_k R^{2-n} + q_k (2^k R)^{2-n} = |b|^{2-n}
    \]

    Since $q_k \in [0,1]$, 
    \[
        \lim_{k \to \infty} p_k = P^b[T_K < \infty] = \left( \frac{|b|}{R} \right)^{2-n}
    \]
    i.e., Brownian motion is \textbf{transient} in $\textbf{R}^n$ for $n > 2$. 
\end{example}

\section{The Characteristic Operator}

We now introduce an operator closely related to the generator, which is often more suitable, such as in the Dirichlet problem.

\begin{definition}[Characteristic Operator]
    Let $\{ X_t \}$ be an Itô diffusion. The \textbf{characteristic operator} $\mathfrak{A} = \mathfrak{A}_X$ of $\{ X_t \}$ is defined by 
    \begin{equation}\label{eq:202301271438}
        \mathfrak{A}~f(x) = \lim_{U \downarrow x} \frac{\mathbb{E}^x [ f(X_{\tau_U})] - f(x)}{\mathbb{E}^x [\tau_U]}, \quad x \in \textbf{R}^n
    \end{equation}
    where each $U$ is an open set $U_k$ decreasing to the point $x$ in the sense that $U_{k+1} \subset U_k$ and $\bigcap_k U_k = \{ x \}$, and $\tau_U = \inf \{ t > 0 : X_t \notin U \}$ is the first exit time from $U$ for $X_t$.

    The set of functions $f : \textbf{R}^n \longrightarrow \textbf{R}$ such that the limit \eqref{eq:202301271438} exists for all $x \in \mathbf{R}^n$ and all $\{ U_k \}$ is denoted by $\mathfrak{D}_{\mathfrak{A}}$.

    If $\mathbb{E}^x[\tau_U] = \infty$ for all open sets $U \ni x$, we define $\mathfrak{A}~f(x) = 0$.
\end{definition}

In fact, $\mathfrak{D}_{A} \subseteq \mathfrak{D}_{\mathfrak{A}}$ and 
\[
    A~f = \mathfrak{A}~f \quad \forall f \in \mathfrak{D}_A
\]
i.e., the characteristic operator is a generalization of the generator.

We will prove that $\mathfrak{A}_X$ and $L_X$ coincide on $\mathfrak{C}^2$. Before that, we need a property of exit times. 

\begin{definition}[Trap]
    A point $x \in \mathbf{R}^n$ is called a \textbf{trap} for $\{ X_t \}$ if 
    \[
        Q^x[\{ X_t = t \text{ for all } t \}] = 1
    \]

    Put another way, $x$ is a trap iff. $\tau_{\{x\}} = \infty$ a.s. w.r.t. $Q^x$. 
\end{definition}

For example, if $\mu(x_0) = \sigma(x_0) = 0$, then $x_0$ is a trap for $X_t$ by strong uniqueness of $X_t$. The intuition here is that the process never leaves the point. 

\begin{lemma}\label{lm:202301271449}
    If $x$ is not a trap for $X_t$, then there exists an open set $U$ containing $x$ such that $\mathbb{E}^x[\tau_U] < \infty$. 
\end{lemma}

In words, if the point is not a trap, there exists an open set containing it such that the process starting at it leaves it in finite time. 

\begin{theorem}
    If $f \in \mathfrak{C}^2$, then $f \in \mathfrak{D}_A$ and $\mathfrak{A}~f = L~f$.
\end{theorem}

\begin{proof}
    First, we need to show that if $x$ is a trap for $\{ X_t \}$, then $\mathfrak{A}~f(x) = 0$. To do that, choose a bounded open set containing $x$. Modifying $f$ to $f_0$ outside $V$ such that $f_0 \in \mathfrak{C}_0^2(\mathbf{R}^n)$, this claim follows. 

    Now, if $x$ is not a trap, choose a bounded open set $U$ containing $x$ such that $\mathbb{E}^x[\tau_U] < \infty$. Applying Dynkin's formula and using that $L~f$ is continuous, we have the result. 
\end{proof}

\begin{remark}
    From the discussions in this chapter, we obtained that an Itô diffusion is
    \begin{enumerate}
        \item Continuous;
        \item A strong Markov process; 
        \item Such that the domain of definition of its characteristic operator includes $\mathfrak{C}^2$.
    \end{enumerate}

    This means that an Itô diffusion is a diffusion in the sense of Dynkin. 
\end{remark}

\section{The Feynman-Kac Formula}

Relates stochastic differential equations and partial differential equations. Special case: the relationship between geometric Brownian motion and Black-Scholes-Merton model.

\begin{theorem}[Feynman-Kac]\label{thm:feynman-kac}
    Consider the stochastic differential equation 
    \begin{equation}\label{eq:202305271535}
        \mathrm{d}X_t = \mu(t, X_t) \mathrm{d}t + \sigma(t, X_t)\mathrm{d}B_t
    \end{equation}

    Let $h(y)$ be a Borel-measurable function and define 
    \begin{equation}\label{eq:202305271536}
        g(t, x) = \mathbb{E}^{t, x} [h(X_T)]
    \end{equation}
    which we suppose to satisfy $\mathbb{E}^{t, x} [|h(X_T)|] < \infty$ for all $t$ and $x$. 

    Then $g(t, x)$ satisfies the partial differential equation 
    \begin{equation*}
        \frac{\partial g}{\partial t}(t, x) + \mu(t, x) \frac{\partial g}{\partial x}(t, x) + \frac{1}{2} \sigma^2(t, x) \frac{\partial^2 g}{\partial x^2}(t, x) = 0
    \end{equation*}
    and the terminal condition 
    \begin{equation*}
        g(T, x) = h(x), \quad \forall ~x
    \end{equation*}
\end{theorem}

\begin{lemma}\label{lm:202305271614}
    Let $X_u$ be a solution to the stochastic differential equation \eqref{eq:202305271535} with initial condition given at $t = 0$. And let $h(y)$ be a Borel-measurable function, $T > 0$, and $g(t,x)$ given by \eqref{eq:202305271536}. Then the stochastic process $g(t, X_t)$, $0 \le t \le T$, is a martingale.
\end{lemma}

\begin{proof}
    Let $0 \le s \le t \le T$. By the \hyperref[thm:markov_prop_ito]{Markov Property for Itô Diffusions}, 
    \[
        \mathbb{E}[h(X_T) \mid \mathfrak{F}_s] = g(s, X_s), \quad \mathbb{E}[h(X_T) \mid \mathfrak{F}_t] = g(t, X_t)
    \]

    Taking conditional expectation of the second equation and using iterated expectation,
    \begin{equation*}
        \begin{aligned}
            \mathbb{E}[g(t, X_t) \mid \mathfrak{F}_s] & = \mathbb{E}[\mathbb{E}[h(X_T) \mid \mathfrak{F}_t] \mid \mathfrak{F}_s] \\
            &= \mathbb{E}[h(X_T) \mid \mathfrak{F}_s] = g(s, X_s)
        \end{aligned}
    \end{equation*}
\end{proof}

Now, we're ready to prove \hyperref[thm:feynman-kac]{Feynman-Kac}.

\begin{proof}
    \textbf{First step: Find the martingale}.

    Let $X_t$ be the solution to the stochastic differential equation \eqref{eq:202305271535} starting at zero. 

    Since $g(t, X_t)$ is a martingale (by the lemma \ref{lm:202305271614}), the term $\mathrm{d}t$ must be zero.
    
    \textbf{Second step: take the differential}.
    
    Thus (omitting the argument $(t, X_t)$), 
    \begin{equation*}
        \begin{aligned}
            \mathrm{d}g &= g_t \mathrm{d}t + g_x \mathrm{d}X + \frac{1}{2}g_{xx} \mathrm{d}X \mathrm{d}X \\
            &= g_t \mathrm{d}t + \mu g_x \mathrm{d}t + \sigma g_x \mathrm{d}B_t + \frac{1}{2} \sigma^2 g_{xx} \mathrm{d}t \\
            &= \left( g_t + \mu g_x + \frac{1}{2} \sigma^2 g_{xx} \right) \mathrm{d}t + \sigma g_x \mathrm{d}B_t
        \end{aligned}
    \end{equation*}

    \textbf{Third step: set the $\mathrm{d}t$ term equal to zero}.

    Setting $\mathrm{d}t$ to zero, 
    \[
        g_t + \mu g_x + \frac{1}{2} \sigma^2 g_{xx} = 0
    \]
    along every path of $X$. 
\end{proof}

\begin{theorem}[Discounted Feynman-Kac]\label{thm:discounted_fk}
    Consider the stochastic differential equation 
    \begin{equation*}
        \mathrm{d}X_t = \mu(t, X_t) \mathrm{d}t + \sigma(t, X_t)\mathrm{d}B_t
    \end{equation*}

    Let $h(y)$ be a Borel-measurable function and $r$ be a constant. Fix $T > 0$ and let $t \in [0,T]$. Define 
    \begin{equation*}
        g(t, x) = \mathbb{E}^{t, x} [e^{-r(T-t)} h(X_T)]
    \end{equation*}
    which we suppose to satisfy $\mathbb{E}^{t, x} [|h(X_T)|] < \infty$ for all $t$ and $x$. 

    Then $g(t, x)$ satisfies the partial differential equation 
    \begin{equation*}
        \frac{\partial g}{\partial t}(t, x) + \mu(t, x) \frac{\partial g}{\partial x}(t, x) + \frac{1}{2} \sigma^2(t, x) \frac{\partial^2 g}{\partial x^2}(t, x) = r g(t,x)
    \end{equation*}
    and the terminal condition 
    \begin{equation*}
        g(T, x) = h(x), \quad \forall ~x
    \end{equation*}
\end{theorem}

\begin{proof}
    As in the previous proof, we let $X_t$ be the solution of the stochastic differential equation starting at zero.

    The difference here is that $g(t, X_t)$ is not a martingale. For $0 \le s \le t \le T$, we have 
    \begin{equation*}
        \begin{aligned}
            \mathbb{E}[g(t, X_t) \mid \mathfrak{F}_s] &= \mathbb{E}[\mathbb{E}[e^{-r(T-t)} h(X_T) \mid \mathfrak{F}_t] \mid \mathfrak{F}_s] \\
            &= \mathbb{E}[e^{-r(T-t)} h(X_T) \mid \mathfrak{F}_s]
        \end{aligned}
    \end{equation*}
    However,
    \begin{equation*}
        g(s, X_s) = \mathbb{E}[e^{-r(T-s)} h(X_T) \mid \mathfrak{F}_s]
    \end{equation*}
    
    The idea now is to `complete the discounting', since the random variable should not depend on $t$. Notice that 
    \begin{equation*}
        e^{-rt} g(t, X_t) = \mathbb{E}[e^{-rT} h(X_T) \mid \mathfrak{F}_t]
    \end{equation*}

    We apply iterated conditioning to show that $e^{-rt} g(t, X_t)$ is a martingale:
    \begin{equation*}
        \begin{aligned}
            \mathbb{E}[e^{-rt} g(t, X_t) \mid \mathfrak{F}_s] &= \mathbb{E}[\mathbb{E}[e^{-rT} h(X_T) \mid \mathfrak{F}_t] \mid \mathfrak{F}_s] \\
            &= \mathbb{E}[e^{-rT} h(X_T) \mid \mathfrak{F}_s] \\
            &= e^{-rs}g(s, X_s)
        \end{aligned}
    \end{equation*}

    Computing the differential,
    \begin{equation*}
        \begin{aligned}
            \mathrm{d}(e^{-rt} g(t, X_t)) &= e^{-rt} \left[ -r g \mathrm{d}t + \frac{\partial g}{\partial t} \mathrm{d}t + \frac{\partial g}{\partial x} \mathrm{d}X_t + \frac{1}{2} \frac{\partial^2 g}{\partial x^2} (\mathrm{d}X_t)^2 \right] \\
            &= e^{-rt}\left[ -r g + \frac{\partial g}{\partial t} + \mu \frac{\partial g}{\partial x} + \frac{1}{2} \sigma^2 \frac{\partial^2 g}{\partial x^2} \right] \mathrm{d}t + e^{-rt} \sigma \frac{\partial g}{\partial x} \mathrm{d}B_t
        \end{aligned}
    \end{equation*}

    Set $\mathrm{d}t$ to zero and we have our result.
\end{proof}

% \section*{Without Section}

% \begin{definition}[Transition Probability Function]
    
% \end{definition}

% Transition Probability Function satisfies the Chapman-Kolmogorov Equation. The proof follows from the law of total probability (REF) by conditioning on all possible values of the process at time u, with $s < u < t$. 

% Moreover, `any function that satisfies the Chapman-Kolmogorov Equation and is a distribution function in $y$ for fixed values of the other arguments, is a transition function of some Markov process'.

% Example: if CDF is of the normal distribution, the corresponding diffusion process is Brownian motion.

% If the SDE has strong solution, the transition probability function can be found as a solution to forward of backward PDEs (see Klebaner's 5.8). 

% For SDEs without strong solutions, the transition function may exist. Turns out that this function determines a Markov process uniquely, which is the weak solution discussed at the end of the previous chapter. See Klebaner 5.7-5.8.

% Isolate the Itô integral in the solution process to obtain the martingale. 

% Theorem: If $X_t$ satisfies the criteria for existence and uniqueness of strong solutions, and $|X_0|$ has a moment generating function $\mathbb{E}[e^{u|X_0|}] < \infty$ for all real $u$, then $|X_t|$ also does, i.e., $\mathbb{E}[e^{u|X_t|}] < \infty$ for all $t \geq 0$, then we can obtain a martingale from the SDE as previously. 

% Corollary: If $f(x,t)$ solves the backward equation 
% \[
%     L_t ~f(x,t) + \frac{\partial f}{\partial t}(x,t) = 0
% \]
% and the conditions of the theorem holds, then $f(X_t, t)$ is a martingale. 

% Klebaner's example 6.3
