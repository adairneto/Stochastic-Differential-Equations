\chapter{Probability Theory}

\section{Probability Spaces}

A probability space starts with a set $\Omega$ called the \textbf{sample space}, which is, intuitively, a list of all possible outcomes of an experiment. Each $\omega \in \Omega$ is a \textbf{sample point}, and each subset $A \subseteq \Omega$ is an \textbf{event}.

To filter `well-behaved' subsets, where it will be possible to measure a probability, the following definition is necessary.

\begin{definition}[$\sigma$-algebra]
	If $\Omega$ is a set, then a \textbf{$\sigma$-algebra} $\mathfrak{F}$ on $\Omega$ is a family of subsets of $\Omega$ satisfying:
	\begin{enumerate}
		\item $\emptyset \in \mathfrak{F}$.
		\item If $A \in \mathfrak{F}$, then $A^c \in \mathfrak{F}$.
		\item If $A_1, A_2, \ldots \in \mathfrak{F}$, then $\bigcup_{i=1}^\infty A_i \in \mathfrak{F}$.
	\end{enumerate}
	The pair $(\Omega, \mathfrak{F})$ is said to be a \textbf{measurable space}, and any subset $B \subseteq \Omega$ that also belongs to $\mathfrak{F}$ is called a \textbf{measurable set}.
\end{definition}

\begin{example}[Examples of $\sigma$-algebras]
	The following are $\sigma$-algebras.
	\begin{enumerate}
		\item The family $\{\emptyset, \Omega\}$ is called the \textbf{trivial $\sigma$-algebra}, and is the smallest one possible.
		\item The power set $\mathfrak{P}(\Omega)$ is called the \textbf{discrete $\sigma$-algebra}, and is the largest one possible.
		\item The family $\{\emptyset, \Omega, A, A^c\}$ is the \textbf{$\sigma$-algebra generated by the set $A$} and is usually denoted by $\mathfrak{F}_A$.
	\end{enumerate}
\end{example}

But the most important $\sigma$-algebra for probability theory is the Borel $\sigma$-algebra, which will be denoted by $\mathfrak{B}$. Taking $\Omega = \mathbb{R}$, the Borel $\sigma$-algebra is generated by the intersection of all $\sigma$-algebras containing the real line intervals.

Notice that the Borel $\sigma$-algebra contains all open sets, closed sets, and all their countable operations with union $\cup$, intersection $\cap$, and their complements $^c$. This is the smallest $\sigma$-algebra containing all open subsets.

More generally, if $U$ is the collection of all open subsets of a topological space $\Omega$, then $\mathfrak{B}$ is called the \textbf{Borel $\sigma$-algebra} on $\Omega$. The elements $B \in \mathfrak{B}$ are called \textbf{Borel sets}.

Given a measurable space, it's possible to assign each outcome to a probability.

\begin{definition}[Probability Measure]
	Let $(\Omega, \mathfrak{F})$ be a measurable space. A \textbf{probability measure} $P$ is the function
	\[
		P : \mathfrak{F} \longrightarrow [0,1]
	\]
	satisfying
	\begin{enumerate}
		\item $P(\emptyset) = 0$ and $P(\Omega) = 1$.
		\item ($\sigma$-additivity). If $A_1, A_2, \ldots \in \mathfrak{F}$ and $A_i \cap A_j = \emptyset$, for $i \neq j$ (i.e. are mutually exclusive), then \[ P \left( \bigcup_{i=1}^\infty A_i \right) = \sum_{i=1}^\infty P(A_i)\]
	\end{enumerate}
	
	And the triple $(\Omega, \mathfrak{F}, P)$ is called a \textbf{probability space}.
\end{definition}

In order to restrict the functions to sets in the $\sigma$-algebra, the following definition will be needed.

\begin{definition}[$\mathfrak{F}$-measurable function]
	Given a probability space $(\Omega, \mathfrak{F}, P)$, a function $Y$ from the sample space $\Omega$ to $\textbf{R}^n$ is called $\mathfrak{F}$-measurable if
	\[
		Y^{-1}(U) = \{ \omega \in \Omega : Y(\omega) \in U\} \in \mathfrak{F}
	\]
	for all open sets $U \in \textbf{R}^n$ (i.e. for all Borel sets $U \subseteq \textbf{R}^n$). In other words, the inverse image of $U$ is in the $\sigma$-algebra.
\end{definition} 

And to attach numerical values to each $\omega \in \Omega$, define the following function.

% Define complete probability space

\begin{definition}[Random variable]
	An $\mathfrak{F}$-measurable function $X: \Omega \longrightarrow \textbf{R}^n$ is called a \textbf{random variable} on a complete probability space $(\Omega, \mathfrak{F}, P)$. 
\end{definition}

This definition means that if we know which event $U$ in the $\mathfrak{F}$ has occurred, then we know which value of $X$ has occurred. 

Consider the measurable space $(\Omega, \mathfrak{P}(\Omega))$, and let $X$ be a random variable with values $x_i$, $i = 1,2, \ldots, k$. The sets
\[
	A_i = \{ \omega \in \Omega : X(\omega) = x_i \} \subseteq \Omega
\]
form a partition of $\Omega$, and the $\sigma$-algebra generated by this partition is called the \textbf{$\sigma$-algebra generated by $X$}.

Notice that this is the smallest $\sigma$-algebra that contains all the sets $A_i$. This $\sigma$-algebra is often denoted by $\mathfrak{F}_X$. Intuitively, this $\sigma$-algebra represents all information available about the sample point $\omega$ by observing $X$. 

% A random variable induces a probability measure. 

\section{Distribution and Density of a Random Variable}

However, given only the probability measure $P$, it is not always immediate how to compute the probability of a given interval, set or value. For that purpose, the following three definitions will be useful.

\begin{definition}[Distribution]
	The \textbf{distribution} of a random variable $X$ is a function $\mu_X : \Omega \longrightarrow \textbf{R}^n$ defined as
	\[
		\mu_X(B) = P [X^{-1}(B)] = P[X \in B] = P(\{ \omega : X(\omega) \in B \})
	\]
\end{definition}

\begin{definition}[Distribution Function]
	The \textbf{cumulative distribution function} (CDF) of a random variable $X$ is defined as
	\[
		F_X(x) = P [X \leq x] = P(\{ \omega : X(\omega) \leq x \})
	\]
	where $x \in \textbf{R}$. Notice that $F$ is non-decreasing, and right-continuous, and approaches $0$ at $- \infty$ and $1$ at $+\infty$.
\end{definition}

\begin{definition}[Joint Distribution]
	If $X$ and $Y$ are random variables, then their \textbf{joint distribution function} is
	\[
		F_{X,Y}(x,y) = P [X \leq x, Y \leq y]
	\]
	where $x,y \in \textbf{R}$.
\end{definition}

The distribution function gives the probability that the random variable $X$ is on the interval $(-\infty, x]$. This also allows to look for an interval $[a,b]$ using $F_X(b) - F_X(a)$.

While the distribution of $X$ returns the probability of an event $\{ \omega : X(\omega) \in B\}$.

\begin{definition}[Density function]
	A random variable $X$ has a \textbf{probability density function} (PDF) $f : \textbf{R}^n \longrightarrow \textbf{R}$ if $f$ is a measurable function and \[ F_X(x) = P[X \leq x] = \int_{-\infty}^x f(y) \, \mathrm{d}y \]
\end{definition}

\section{Expected Value and Variance}

Another useful tool for any given random variable is to know its mean value and how much it varies. This motivates the following.

\begin{definition}[Expectation]
	Let $(\Omega, \mathfrak{F}, P)$ be a probability space and $X$ a random variable. If $\int_{\Omega} |X(\omega)|\, \mathrm{d}P(\omega) < \infty$, then the \textbf{expected value} (or \textbf{mean value} of $X$ with respect to $P$ is
	\[
		\mathbb{E}[X] = \int_{\Omega} X(\omega) \, \mathrm{d}P(\omega) = \int_{\textbf{R}^n} x \, \mathrm{d}\mu_X(x)
	\]
\end{definition}

Notice that the expectation is linear. I.e., if $X$ and $Y$ are integrable and $a$ and $b$ are constants, then \[ \mathbb{E} [aX+bY] = a \mathbb{E}[X] + b \mathbb{E}[Y] \]

If $f$ is a Borel measurable function and $\int_{\Omega} |f(X)| \, \mathrm{d} P(\omega) < \infty$, then
\[
	\mathbb{E}[f(X)] = \int_{\Omega} f(X(\omega)) \, \mathrm{d} P(\omega) = \int_{\textbf{R}^n} f(X) \, \mathrm{d}\mu_X(x)
\]

\begin{theorem}[Chebychev's inequality]
	\[
		P[ |X| \geq \lambda] \leq \frac{1}{\lambda^p} \mathbb{E}[|X|^p]
	\]
	for all $\lambda \geq 0$.
\end{theorem}

\begin{definition}[Variance]
	Let $\mu := \mathbb{E}[X]$, i.e., the expected value of a random variable $X$. Then the \textbf{variance} of $X$ is given by
	\[
		\text{Var}[X] = \mathbb{E}[\|X - \mu\|^2] = \int_{\Omega} |X - \mu|^2 \, \mathrm{d} P(\omega)
	\]
\end{definition}

\begin{definition}[Covariance]
	Let $X$ and $Y$ be integral random variables. Let $\mu_X = \mathbb{E}[X]$ and $\mu_Y = \mathbb{E}[Y]$. If $XY$ is integrable, then the \textbf{covariance} of $X$ and $Y$ is:
	\[
		\text{Cov}[X,Y] = \mathbb{E}[(X-\mu_X)(Y-\mu_Y)] = \mathbb{E}[XY]-\mu_X \mu_Y
	\]
	Notice that $\text{Var}[X] = \text{Cov}[X,X]$.
\end{definition}

The variance of $X$ can be computed more simply using the following theorem.

\begin{theorem}
	Let $X$ be a random variable. Then,
	\begin{enumerate}
		\item $\text{Var}[X] = \mathbb{E}[X^2] - \mathbb{E}[X]^2$.
		\item $\text{Var}[aX+b] = a^2 \text{Var}[X]$.
		\item $\text{Cov}[X,Y] = \text{Cov}[Y,X]$.
		\item $\text{Cov}[aX+bY,Z] = a \text{Cov}[X,Z] + b \text{Cov}[Y,Z]$.
		\item $\text{Var}[X+Y] = \text{Var}[X] + \text{Var}[Y] + 2 \text{Cov}[X,Y]$.
	\end{enumerate}		
\end{theorem}

\begin{definition}[Independence]
	Two events $A, B \in \mathfrak{F}$ are \textbf{independent} if
	\[
		P[A \cap B] = P[A]\cdot P[B]
	\]
	
	More generally, any collection of events $A_i$, $i = 1,2,\ldots$, is called independent if for any $n \in \textbf{N}$ and any choice of indices $i_k$, $k = 1, 2, \ldots$, 
	\[
		P \left( \bigcap_{k=1}^n A_{i_k} \right) = \prod_{k=1}^n P\left( A_{i_k} \right)
	\]
\end{definition}

\begin{definition}[Conditional Expectation]
	Let $X$ and $Y$ be random variables. Then, given that $f_Y(y) > 0$, the \textbf{conditional distribution} of $X$ given $Y=y$ is given by the conditional density
	\[
		f(x|y) = \frac{f(x,y)}{f_Y(y)}
	\]
	
	And the \textbf{conditional expectation} of $X$ given $Y=y$ is
	\[
		\mathbb{E} [X| Y = y] = \int_{-\infty}^\infty x f(x|y) \, \mathrm{d}x
	\]
\end{definition}

The intuition behind this definition is to build an estimate of the random variable $X$ given the information available in $Y$.

Some important and very useful properties of conditional expectation are listed below.

\begin{theorem}
	Let $X, Y : \Omega \longrightarrow \textbf{R}^n$ be random variables with $\mathbb{E}[|X|] < \infty$ and $\mathbb{E}[|Y|] < \infty$, $\mathfrak{H} \subseteq \mathfrak{F}$ a $\sigma$-algebra, and let $a, b \in \textbf{R}$.
	\begin{enumerate}
		\item $\mathbb{E} [ a X + b Y | \mathfrak{H}] = a \mathbb{E} [X | \mathfrak{H}] + b \mathbb{E} [Y | \mathfrak{H}]$.
		\item $\mathbb{E} [ \mathbb{E} [X | \mathfrak{H}] ] = \mathbb{E} [X]$.
		\item $\mathbb{E} [X | \mathfrak{H}] = X$ if $X$ is $\mathfrak{H}$-measurable.
		\item $\mathbb{E} [X | \mathfrak{H}] = \mathbb{E} [X]$ if $X$ is independent of $\mathfrak{H}$.
		\item $\mathbb{E} [Y \cdot X | \mathfrak{H}] = Y \cdot \mathbb{E} [X | \mathfrak{H}]$ if $Y$ is $\mathfrak{H}$-measurable.
		\item If $\mathfrak{G} \subseteq \mathfrak{H}$ is a $\sigma$-algebra, then \[ \mathbb{E} [X | \mathfrak{G}] = \mathbb{E} [ \mathbb{E} [X | \mathfrak{H}] | \mathfrak{G}] \]
		\item If $\varphi : \textbf{R} \longrightarrow \textbf{R}$ is convex and $\mathbb{E} [ | \varphi (X) | ] < \infty$, then \[ \varphi (\mathbb{E} [X | \mathfrak{H}]) \leq \mathbb{E} [\varphi(X) | \mathfrak{H}] \]
	\end{enumerate}
\end{theorem}

\begin{definition}[Infinitely often]
If $A_1, A_2, \ldots, A_n, \ldots$ are events in the probability space, then the event
\[
	\bigcap_{n=1}^\infty \bigcup_{m=n}^\infty A_m = \{ \omega \in \Omega : \omega \text{ belongs to infinitely many of the } A_n \}
\]
is called \textbf{$A_n$ infinitely often}, or simply `\textbf{$A_n$ i.o.}'.
\end{definition}

The next lemma helps us check if some sequence of events occurs infinitely often.

\begin{lemma}[Borel-Cantelli]
	If $\sum_{n=1}^\infty P(A_n) < \infty$, then
	\[
		P(A_n \text{ i.o.}) = P \left( \bigcap_{n=1}^\infty \bigcup_{m=n}^\infty A_m \right) = 0
	\]
\end{lemma}

% The limit of random variables and a composition of r.v. are r.v.

The most important distribution in this text is the \textbf{normal} (or \textbf{gaussian}) distribution.
 
\begin{definition}[Normal Distribution]
	If random variable $X$ has mean $\mu$, variance $\sigma^2$ and a density function of the form
	\[
		f(x) = \frac{1}{\sqrt{2\pi} \sigma} \exp \left( \frac{-(x-\mu)^2}{2\sigma^2} \right) 
	\]
	then $X$ is said to have a \textbf{normal distribution}, which we denote by $X \sim N(\mu, \sigma^2)$.
\end{definition}

Notice that $X$ can be normalized (i.e. transformed into a distribution of the form $N(0,1)$) by taking $Z = \frac{X-\mu}{\sigma}$.

%If $X \sim N(\mu, \sigma^2)$, then its moment generating function is
%\[
%	m(t) = \mathbb{E}[e^{tX}] = \int_{-\infty}^{\infty} e^{tx}f(x)dx
%\]

\begin{definition}[Lipschitz and H\"older Conditions]
	A function $f$ is said to be \textbf{H\"older continuous} of order $\alpha$, $0 < \alpha \leq 1$, on an interval $[a,b] \subseteq \textbf{R}$ if there exists a constant $K > 0$, such that for all $x,y \in [a,b]$ we have
	\[
		|f(x) - f(y)| \leq K |x-y|^{\alpha}
	\]
	If $\alpha = 1$, then the function $f$ is said to be \textbf{Lipschitz continuous}.
\end{definition}

\section{Stochastic Processes}

\begin{definition}[Stochastic Process]
	A \textbf{stochastic process} is a collection of random variables $\{ X(t) \}$ parametrized by time $t \in T$.  
	
	For each point $\omega \in \Omega$, the mapping $t \longrightarrow X_t(\omega)$ is the respective \textbf{sample path}, also called \textbf{realization} or \textbf{trajectory}.
\end{definition}

\begin{definition}[Filtration]
	A filtration $\textbf{F}$ is a collection of $\sigma$-algebras
	\[
		\textbf{F} = \{ \mathfrak{F}_0, \mathfrak{F}_1, \ldots, \mathfrak{F}_t, \ldots, \mathfrak{F}_T \}
	\]
	such that $\mathfrak{F}_t \subseteq \mathfrak{F}_{t+1}$.
\end{definition}

The idea behind this definition is to model the flow of information. As the time $t$ passes, the observer has more information, and does not lose any previous data. Notice that this implies finer partitions of the sample space $\Omega$.

Given a stochastic process $\{ X(t) \}$, let $\mathfrak{F}_t$ be the $\sigma$-algebra generated by the random variables $X_s$, $s = 0, \ldots, t$. Since $\mathfrak{F}_t \subseteq \mathfrak{F}_{t+1}$, these $\sigma$-algebras form a filtration called the \textbf{natural filtration} of the process $\{ X(t) \}$, and contain all available information of the process up to the time $t$.

\begin{definition}[Martingale]
	A stochastic process $\{M_t \}$ on $(\Omega, \mathfrak{F}, P)$ is called a \textbf{martingale} with respect to a filtration $\mathfrak{M}_t$ if 
	\begin{enumerate}
		\item $M_t$ is $\mathfrak{M}_t$-measurable for all $t$.
		\item $\mathbb{E}[|M_t|] < \infty$ for all $t$.
		\item $\mathbb{E}[M_s | \mathfrak{M}_t] = M_t$ for all $s \geq t$.
	\end{enumerate}
\end{definition}

Intuitively, a martingale is a stochastic process in which the future has no tendency to go up or down. In other words, the expected value of any time in the future is equal to the value of the process at the present time.

\begin{theorem}[Doob's martingale inequality]
	If $M_t$ is a martingale such that the mapping $t \longrightarrow M_t(\omega)$ is continuous a.s., then for all $p \geq 1$, $T \geq 0$ and $\lambda > 0$,
	\[
		P \left[ \sup_{0 \leq t \leq T} |M_t| \geq \lambda \right] \leq \frac{1}{\lambda^p} \mathbb{E} [|M_T|^p]
	\]
\end{theorem}

\section{Brownian Motion}

In 1828 the Scottish botanist Robert Brown described the irregular motion of pollen grains suspended in fluid, that is now known as the Brownian Motion. To describe this mathematically, we use a stochastic process $B_t(\omega)$, which is the position at time $t$ of the particle $\omega$.

This can also be understood as the model for the cumulative effect of `noise'. And is also called Wiener process, after N. Wiener, who formalized mathematically the Brownian motion.

\begin{definition}[Brownian Motion]
	A Brownian motion $\{ B(t) \}$ is a stochastic process with the following properties:
	\begin{enumerate}
		\item (Independence of increments). $B(t) - B(s)$, for $t > s$, are independent. That means that the direction that the particle will go does not depend on the past.
		\item (Normal increments). $B(t) - B(s)$ has normal distribution with mean $0$ and variance $t - s$. Notice that by taking $s = 0$ we have $B(t) - B(0) \sim N(0,t)$.
		\item (Continuity of paths). $B(t)$, $t \geq 0$, are continuous functions of $t$. 
	\end{enumerate}
\end{definition}

The existence and continuity of Brownian motion is proved using Kolmogorov's extension and continuity theorem \cite{oksendal2013stochastic}.

\begin{theorem}[Quadratic variation]
	If $t \geq s$,
	\[
		\mathbb{E}[(B_t - B_s)^2] = t - s
	\]
	
	More generally (in $\textbf{R}^n$):
	\[
		\mathbb{E}[(B_t - B_s)^2] = n(t - s)
	\]
\end{theorem}


\begin{theorem}
	\[
		\mathbb{E}[f(B_t)] = \frac{1}{\sqrt{2 \pi t}} \int_{\textbf{R}} f(x) e^{-\frac{x^2}{2t}} \, \mathrm{d}x
	\]
\end{theorem}

\begin{theorem}
	The expected value of odd moments of $B_t$ (other than one) are zero. And the even moments are given by
	\[
		\mathbb{E}[B_t^{2k}] = \frac{(2k)!}{2^k k!} t^k
	\]
\end{theorem}

Last but not least, two important identities, useful for further calculations are:
\[
	B_j (B_{j+1} - B_j) = \frac{1}{2} [B_{j+1}^2 - B_j^2 - (B_{j+1} - B_j)^2]
\]
\[
	B_j^2 (B_{j+1} - B_j) = \frac{1}{3} (B_{j+1}^3 - B_j^3) - B_j(B_{j+1} - B_j)^2 - \frac{1}{3}(B_{j+1} - B_j)^3
\]

\section{Riemann-Stieltjes Integral}

\begin{definition}[Riemann-Stieltjes Integral]
	If $\varphi$ is a continuous function on $[a,b]$ and $F$ is a distribution function, we define the \textbf{Riemann-Stieltjes Integral} of $\varphi$ on $[a,b]$ in relation to $F$ (or weighted by $F$) as
	\[
		\lim_{\| \Delta \| \to 0} \sum_{i=1}^n \varphi(y_i)[F(x_{i+1}) - F(x_i))] = \int_a^b \varphi(x) ~\mathrm{d}F(x)
	\]
	where $a = x_1 < x_2 < \ldots < x_{n+1} - b$, $y_i$ is an arbitrary point of $[x_i, x_{i+1}]$, and $\| \Delta \| = \max_{1 \leq i \leq n} (x_{i+1} - x_i)$.
\end{definition}

If $F$ is a discrete random variable, then
\[
	\int_a^b \varphi ~\mathrm{d}F = \sum_{i: a < x_i \leq b} \varphi(x_i) p(x_i)
\]

And if $F$ is a continuous random variable, then
\[
	\int_a^b \varphi ~\mathrm{d}F = \int_a^b \varphi(x) f(x) ~\mathrm{d}x
\]

\section{Modes of Convergence}

\begin{definition}
	A \textbf{space $L^p (\Omega, \mathfrak{F}, P)$} is the space of measurable functions in which the $p$-th power of the absolute value is Lebesgue integrable. I.e., functions of the form
	\[
		\| f \|_p := \left( \int_\Omega |f|^p \, \mathrm{d}\mu \right)^{1/p} < \infty
	\]

	We can also write this as
	\[
		\| X \|_p := \sqrt[p]{\mathbb{E}[|X|^p]} < \infty
	\]
\end{definition}

A sequence of random variable can converge in different ways. The following definitions are given in an increasing order of strength.

\begin{definition}[Convergence in Distribution]
	A sequence of random variables $\{ X_n \}$ \textbf{converges in distribution} to $X$ if their distribution functions $F_{X_n}(x)$ converge to the distribution function $F_X(x)$ at any point of continuity of $F_X$.
	
	In other words, for any bounded function $f : \textbf{R} \longrightarrow \textbf{R}$ we have
	\[
		\lim_{n \to \infty} \mathbb{E}[f(X_n)] = \mathbb{E}[f(X)]
	\]
	I.e.,
	\[
		\lim_{n \to \infty} \int_{\Omega_n} f(X_n) \, \mathrm{d}P_n = \int_{\Omega} f(X) \, \mathrm{d}P
	\]
\end{definition}

\begin{definition}[Convergence in Probability]
	A sequence of random variables $\{ X_n \}$ \textbf{converges in probability} to $X$ if for any $\varepsilon > 0$, we have
	\[
		\lim_{n \to \infty} P[|X_n(\omega) - X(\omega)| > \varepsilon] = 0
	\]
\end{definition}

\begin{definition}[Convergence Almost Surely]
	A sequence of random variables $\{ X_n \}$ \textbf{converges almost surely (a.s.)} to $X$ if for any $\omega$ outside a set of probability zero we have that
	\[
		\lim_{n \to \infty} X_n(\omega) = X(\omega)
	\]
	Alternatively, we can write
	\[
		P[X_n(\omega) \to X(\omega)] = 1
	\]
\end{definition}

\begin{definition}[$L^p$-Convergence]
	A sequence of random variables $\{ X_n \}$ \textbf{converges in $L^p$} to $X$ if $\{X_n\} \subseteq L^p$, $p \in [1,\infty)$, and
	\[
		\lim_{n \to \infty} \|X_n - X\|_p = 0
	\]
	I.e., $\mathbb{E} \left[ |X_n|^p \right] < \infty$ and
	\[
		\lim_{n \to \infty} \mathbb{E} \left[ |X_n - X|^p \right] = 0
	\]
\end{definition}